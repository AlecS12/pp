from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import matplotlib 
matplotlib.use('agg')
import matplotlib.pyplot as plt

from datetime import datetime
import time,pickle,os.path,sys
import tensorflow as tf

import statsmodels.api as sm
import scipy.stats as stats

from generation import *
from BatchIterator import PaddedDataIterator
from Plotter import get_intensity,get_integral
from Utils import sequence_filter,lambda_estimation,file2sequence,sequence2file


##############################################################################
# parameters
DATA = 'hawkes_gaussian' # hawkes, gaussian, rnn, polynimial

BATCH_SIZE = 256 # Batch size
MAX_STEPS = 300
ITERS = 30000#100000 # how many generator iterations to train for
SEED = 12345 # set graph-level seed to make the random sequences generated by all ops be repeatable across sessions
D_DIFF = True
MARK = False
ITERATION = 0
DATA = sys.argv[1]
#T = 15.0 # end time of simulation
T = float(sys.argv[3])
SEQ_NUM = 2000 # number of sequences
DIM_SIZE = 1
SEQ_NUM = int(float(sys.argv[2]))
if DATA in ['911calls','hawkes_gaussian','hawkes_poly','mimic','meme','citation','stock',"mixture1","mixture2","mixture3","mixture4"]:
    REAL_DATA = True
else:
    REAL_DATA = False
    
tf.set_random_seed(SEED)
np.random.seed(SEED)

##############################################################################
# prepare data


FILE_NAME = 'pickled_data_{}'.format(DATA)
if not os.path.isfile(FILE_NAME):
    if DATA=='gaussian': #QQ_plot for gaussian is not good as hawkes,selfcorrecting, perhaps that simulating is not good.
        intensityGaussian = IntensitySumGaussianKernel(3,[3,7,11], [1,1,1], [2,3,2])
        real_sequences = generate_sample(intensityGaussian, T, 20000)  
        sequence2file(real_sequences,'gaussian')
    else:
        real_sequences = file2sequence(DATA)

    #lambda0 = np.mean([len(item) for item in real_sequences])/T
    #intensityPoisson = IntensityHomogenuosPoisson(lambda0)
    #fake_sequences = generate_sample(intensityPoisson, T, 20000)
    pickle.dump(real_sequences,open(FILE_NAME,'wb'))
else:
    real_sequences = pickle.load(open(FILE_NAME,'rb'))
  
print ((np.mean([len(item) for item in real_sequences])/T))
if not REAL_DATA:
    real_sequences = real_sequences[:SEQ_NUM]
real_iterator = PaddedDataIterator(real_sequences,T,MARK,D_DIFF)


###############################################################################
# define model

def discriminator(rnn_inputs, #dims batch_size x num_steps x input_size
    seqlen,
    lower_triangular_ones,
    state_size = 64,
    batch_size = BATCH_SIZE,
    scope_reuse=False):
    epilson = tf.constant(1e-3,tf.float32)

    with tf.variable_scope("discriminator") as scope:
        if scope_reuse:
            scope.reuse_variables()
        
        num_steps = tf.shape(rnn_inputs)[1]-1
        y = rnn_inputs[:,1:,0]
        rnn_inputs = rnn_inputs[:,:num_steps,:]
  
        cell = tf.contrib.rnn.BasicRNNCell(state_size)#,activation=tf.nn.relu  
        init_state = cell.zero_state(batch_size, tf.float32)
        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen-1,initial_state=init_state)
 
        #reshape rnn_outputs
        rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])
        y = tf.reshape(y, [-1,1])
        
        # Softmax layer
        with tf.variable_scope('softmax'):
            W = tf.get_variable('W', [state_size, 1])
            wt = tf.get_variable('wt', [1], initializer=tf.constant_initializer(1.0))
            b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0))
        
        wt = tf.cond(tf.less(tf.reshape(tf.abs(wt),[]),epilson),lambda:tf.sign(wt)*epilson, lambda:wt) #put in wrong position before
        part1 = tf.matmul(rnn_outputs, W) + b #output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j]), for all indices i, j.
        part2 = wt*y
        logits = part1 + part2 + (tf.exp(part1)-tf.exp(part1+part2))/wt 
        
        #length of y minus one
        seqlen_mask = tf.slice(tf.gather(lower_triangular_ones, seqlen - 2),[0, 0], [batch_size,num_steps])

        logits = tf.reshape(logits,[batch_size,num_steps])
        logits *= seqlen_mask 
        # Average over actual sequence lengths.
        fval = tf.reduce_sum(logits, axis=1)
        fval = - tf.reduce_mean(fval)
    return fval,wt
  
  


def generator(
    NUM_STEPS = 200,
    state_size = 64,
    step_ = 1,
    T = 15.0,
    scope_reuse=True):

    with tf.variable_scope("discriminator") as scope:
        if scope_reuse:
            scope.reuse_variables()
        
        cell = tf.contrib.rnn.BasicRNNCell(state_size)#,activation=tf.nn.relu 
        state = cell.zero_state(1, tf.float32)
        #rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,initial_state=init_state)
        
        # Softmax layer
        with tf.variable_scope('softmax'):
            W = tf.get_variable('W', [state_size, 1])
            wt = tf.get_variable('wt', [1], initializer=tf.constant_initializer(1.0))
            b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0))

        
        outputs = []
        s = tf.random_uniform([1,1])
        t_ = s
        curr_ = tf.constant(0,tf.float32)
        bool_v = tf.less(1,2)
        cell_output = tf.random_uniform([1,1])
        constant_zero = tf.constant(0,tf.float32)
        def fn1(out_,in_):
            out_.append(in_)
            return constant_zero
          
        for _ in range(NUM_STEPS):
            with tf.variable_scope('rnn'):
            #if bool_v:
                #cell_output, state = cell(s, state)
                #curr_ = 0
            #else:
                #curr_ = curr_ + s
                  cell_output, state = tf.cond(tf.reshape(bool_v,[]), lambda: cell(s,state), lambda:(cell_output,state))
            curr_ = tf.cond(tf.reshape(bool_v,[]), lambda: constant_zero, lambda:curr_+s)
            part1 = tf.matmul(cell_output, W) + b
            intens1 = tf.maximum( tf.exp(part1+wt*curr_), tf.exp(part1+wt*(curr_+5.0)) )
            
            s = tf.contrib.distributions.Exponential(intens1).sample()#todo
            s = tf.reshape(s,[1,1])
            t_ = t_ + s
            
            part2 = wt*(curr_+ s)
            intens2 = tf.exp(part1+part2)
            mu = tf.random_uniform([1,1],0,1)
            bool_v = tf.greater_equal(intens2, mu*intens1)
            #if bool_v:
                #outputs.append(t_)  
            
            tf.cond(tf.reshape(bool_v,[]),lambda:fn1(outputs,t_),lambda:constant_zero)
            

            
                  
        """
        with tf.variable_scope('rnn'):
            for time_step in range(num_steps):
                if time_step < 5: #
                    (cell_output, state) = cell(rnn_inputs[:,time_step,:], state)
                    part1 = tf.matmul(cell_output, W) + b
                    part2 = wt*y
                    logits = tf.exp(part1 + part2 + (tf.exp(part1)-tf.exp(part1+part2))/wt)
                    output = tf.reduce_sum(tf.matmul(logits, tf.reshape(y,[-1,1])),axis=1, keep_dims=True)*interval
                else:
                    (cell_output, state) = cell(output, state)
                    part1 = tf.matmul(cell_output, W) + b
                    part2 = wt*y
                    logits = tf.exp(part1 + part2 + (tf.exp(part1)-tf.exp(part1+part2))/wt)
                    output = tf.reduce_sum(tf.matmul(logits, tf.reshape(y,[-1,1])),axis=1, keep_dims=True)*interval
                    intens.append(tf.exp(part1 + part2)[:,0:1])
                    outputs.append(output)
          """
        
    return tf.reshape(tf.concat(outputs,axis=0),[-1])


X = tf.placeholder(tf.float32, shape=[BATCH_SIZE, None, 1])
real_data = X
    
real_seqlen = tf.placeholder(tf.int32, shape=[BATCH_SIZE])
lower_triangular_ones = tf.constant(np.tril(np.ones([MAX_STEPS,MAX_STEPS])),dtype=tf.float32)

neg_loglikelihood,decay_w =  discriminator(real_data,real_seqlen,lower_triangular_ones)

#Z = tf.placeholder(tf.float32, shape=[SEQ_NUM, fake_batch[0].shape[1], 1])
fake_data = generator()

train_variables = tf.trainable_variables()
discriminator_variables = [v for v in train_variables if v.name.startswith("discriminator")]

print(map(lambda x: x.op.name, train_variables))
print(map(lambda x: x.op.name, discriminator_variables))

column_para = tf.concat([tf.reshape(p,[-1]) for p in discriminator_variables],axis=0)
para_max = tf.reduce_max(tf.abs(column_para))
regularizer = 10*tf.norm(column_para)
D_loss = neg_loglikelihood  + regularizer
# loss explode, why?
# not convex to parameters, don't converge 


disc_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(D_loss, var_list=discriminator_variables)


##################################################################################
#run

saved_file = "rmtpp_{}".format(DATA)
if not os.path.exists('out/%s'%saved_file):
    os.makedirs('out/%s'%saved_file)
n_t = 30
ts_real, intensity_real = get_intensity(real_sequences, T, n_t)

# Add ops to save and restore all the variables.
saver = tf.train.Saver()


gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0, allow_growth=True)
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))

sess.run(tf.global_variables_initializer())

stop_indicator = False
last_value = 0
last_loss = 0
# GAN train
for it in range(ITERS):
    real_batch = real_iterator.next_batch(BATCH_SIZE)
    D_loss_curr, _, neglike,regular,para_max_,decay_w_,column_para_ = sess.run([D_loss,disc_train_op,neg_loglikelihood,regularizer,para_max,decay_w,column_para],
                              feed_dict={ X:real_batch[0], real_seqlen:real_batch[1]})
    if it%1000==0:
        print ('Iter: {}; Data: {}; D loss: {:.4}; neglik:{}; reg:{} Para:{}; w:{}'.format(it, DATA, D_loss_curr,neglike,regular,para_max_,decay_w_))
        if np.max(np.abs(last_value-column_para_))<1e-4 and np.abs(D_loss_curr-last_loss)<1:#np.max(np.abs(last_value-column_para_))<1e-2
            stop_indicator = True
        last_value = column_para_
        last_loss = D_loss_curr

    if it % 1000 == 0 and it>10000:
        sequences_generator = []
        for _ in range(100):
            sequences_gen = sess.run(fake_data)
            sequences_generator.append(sequences_gen)
        sequences_generator = sequence_filter(sequences_generator,None,T)
        ts_gen, intensity_gen = get_intensity(sequences_generator, T, n_t)
        deviation = np.linalg.norm(intensity_gen-intensity_real)/np.linalg.norm(intensity_real)

        plt.plot(ts_real,intensity_real, label='real')
        plt.plot(ts_gen, intensity_gen, label='generated')
        plt.legend(loc=1)
        plt.xlabel('time')
        plt.ylabel('intensity')
        plt.savefig('out/{}/{}_{}.png'
                    .format(saved_file,str(it).zfill(3),deviation), bbox_inches='tight')
        plt.close()
        
        if not REAL_DATA and DATA!="rmtpp":
            integral_intensity = get_integral(sequences_generator, DATA)
            integral_intensity = np.asarray(integral_intensity)
            fig = sm.qqplot(integral_intensity, stats.expon, distargs=(), loc=0, scale=1,line='45')
            res,slope_intercept = stats.probplot(integral_intensity, dist=stats.expon)
            plt.grid()
            fig.savefig('out/{}/{}.png'.format(saved_file,it))
            plt.close()
            
        
        
    if it  == ITERS-1 or stop_indicator: 
        sequences_generator = []
        for _ in range(2000):
            sequences_gen = sess.run(fake_data)
            sequences_generator.append(sequences_gen)
        sequences_generator = sequence_filter(sequences_generator,None,T) # remove padding tokens
        sequence2file(sequences_generator, 'rmtpp_{}_{}_{}'.format(DATA,SEQ_NUM,ITERATION))
        #if not os.path.isfile('data/rmtpp_model_{}_{}.ckpt'.format(DATA,ITERATION)):
        #    save_path = saver.save(sess, 'data/rmtpp_model_{}_{}.ckpt'.format(DATA,ITERATION))
        break
            

